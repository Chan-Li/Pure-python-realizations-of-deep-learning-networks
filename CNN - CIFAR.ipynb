{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1418d1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy as cp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from cupy import log as ln\n",
    "import random\n",
    "from cupy import random\n",
    "import math\n",
    "import scipy.stats as stats\n",
    "import os\n",
    "from matplotlib.pyplot import plot,savefig\n",
    "import sys\n",
    "filePath = 'Utils/cup'\n",
    "sys.path.append(filePath)\n",
    "import os\n",
    "from functions import relu,drelu,softmax,divi_,turn_2_zero,scale,tanh,dtanh,sigmoid,dsigmoid\n",
    "from optimizers import Adam,BN_layer\n",
    "from cpsave import cp_save\n",
    "from model_save import model_save\n",
    "\n",
    "def padding(image, zero_num):\n",
    "    if len(image.shape) == 4:\n",
    "        image_padding = cp.zeros((image.shape[0],image.shape[1]+2*zero_num,image.shape[2]+2*zero_num,image.shape[3]))\n",
    "        image_padding[:,zero_num:image.shape[1]+zero_num,zero_num:image.shape[2]+zero_num,:] = image\n",
    "        return image_padding\n",
    "    elif len(image.shape) == 3:\n",
    "        image_padding = cp.zeros((image.shape[0]+2*zero_num, image.shape[1]+2*zero_num, image.shape[2]))\n",
    "        image_padding[zero_num:image.shape[0]+zero_num, zero_num:image.shape[1]+zero_num,:] = image\n",
    "        return image_padding\n",
    "    else:\n",
    "        print(\"维度错误\")\n",
    "        return 0\n",
    "    \n",
    "\n",
    "\n",
    "def pool(feature, size=2, stride=2):\n",
    "    num,feature_h, feature_w, feature_ch = feature.shape\n",
    "    pool_h = cp.uint16((feature_h - size)/stride + 1)\n",
    "    pool_w = cp.uint16((feature_w - size)/stride + 1)\n",
    "    feature_reshaped = feature.reshape(num,pool_h, feature_h//pool_h, pool_w, feature_w//pool_w, feature_ch)\n",
    "    out = feature_reshaped.max(axis=2).max(axis=3)\n",
    "    out_location_c = feature_reshaped.max(axis=2).argmax(axis=3)\n",
    "    out_location_r = feature_reshaped.max(axis=4).argmax(axis=2)\n",
    "    out_location = out_location_r * size + out_location_c\n",
    "    return out, out_location\n",
    "\n",
    "def rotate180(kernel, axis=(-2, -3)):\n",
    "    return cp.flip(kernel, axis)\n",
    "def add_bias(conv, bias):\n",
    "    if conv.shape[-1] != bias.shape[0]:\n",
    "        print(\"给卷积添加偏置维度出错\")\n",
    "    else:\n",
    "        for i in range(bias.shape[0]):\n",
    "            conv[:,:,:,i] += bias[i,0]\n",
    "    return conv\n",
    "def uni_permu(a1,b1,direction):\n",
    "    a=a1*1\n",
    "    b=b1*1\n",
    "    if direction ==1:\n",
    "        p = cp.random.permutation(len(a.T))\n",
    "        return cp.array((a.T[p]).T), cp.array((b.T[p]).T)\n",
    "    if direction == 0:\n",
    "        p = cp.random.permutation(len(a))\n",
    "        return cp.array((a[p])), cp.array((b[p]))\n",
    "def mini_batch_generate(mini_batch_size,data1,label1):\n",
    "    data = cp.array(data1*1)\n",
    "    label =cp.array(label1*1)\n",
    "    if (data.shape[0]%mini_batch_size == 0):\n",
    "        n=data.shape[0]\n",
    "    else:\n",
    "        n = (int(data.shape[0]/mini_batch_size))*mini_batch_size\n",
    "    data,label = uni_permu(data,label,0)\n",
    "    mini_batches = cp.array([data[k:k+mini_batch_size] for k in range(0,n,mini_batch_size)])\n",
    "    mini_batches_labels =cp.array([label[k:k+mini_batch_size] for k in range(0,n,mini_batch_size)])\n",
    "    mini_batches = mini_batches.reshape(int(data.shape[0]/mini_batch_size),mini_batch_size,data1.shape[1],data1.shape[2],data1.shape[3])\n",
    "    mini_batches_labels = mini_batches_labels.reshape(int(label.shape[0]/mini_batch_size),mini_batch_size,label.shape[1],1)\n",
    "    return (mini_batches),(mini_batches_labels)\n",
    "\n",
    "\n",
    "\n",
    "def conv_cal_b(out_img_delta):\n",
    "    #shape: 500, 10, 10, 16\n",
    "    return cp.average(out_img_delta,axis = (0,1,2)).reshape(out_img_delta.shape[-1],1)\n",
    "\n",
    "        \n",
    "def pool_delta_error_bp(pool_out_delta1, pool_out_max_location1, size=2, stride=2):\n",
    "    pool_out_delta = pool_out_delta1*1\n",
    "    pool_out_max_location = pool_out_max_location1*1\n",
    "    num,pool_h, pool_w, pool_ch = pool_out_delta.shape\n",
    "    in_h = cp.uint16((pool_h-1)*stride+size)\n",
    "    in_w = cp.uint16((pool_w-1)*stride+size)\n",
    "    in_ch = pool_ch\n",
    "    #复原尺寸\n",
    "    pool_out_delta_reshaped = pool_out_delta.transpose(0,3,1,2)#16*5*5\n",
    "    pool_out_delta_reshaped = pool_out_delta_reshaped.reshape(num*pool_h*pool_w*pool_ch)\n",
    "    \n",
    "    pool_out_max_location_reshaped = pool_out_max_location.transpose(0,3,1,2)\n",
    "    pool_out_max_location_reshaped = pool_out_max_location_reshaped.reshape(num*pool_h*pool_w*pool_ch)\n",
    "    \n",
    "    in_delta_matrix = cp.zeros([num*pool_h*pool_w*pool_ch,size*size])\n",
    "    \n",
    "    in_delta_matrix[cp.arange(num*pool_h*pool_w*pool_ch), pool_out_max_location_reshaped] = pool_out_delta_reshaped\n",
    "    \n",
    "    in_delta = in_delta_matrix.reshape(num,pool_ch,pool_h, pool_w, size, size)\n",
    "    in_delta = in_delta.transpose(0,2,4,3,5,1)\n",
    "    in_delta = in_delta.reshape(num,in_h, in_w, in_ch)\n",
    "    return  in_delta\n",
    "\n",
    "\n",
    "\n",
    "def split_by_strides(x, kernel_size, stride=(1, 1)):\n",
    "    \"\"\"\n",
    "    将张量按卷积核尺寸与步长进行分割\n",
    "    :param x: 被卷积的张量\n",
    "    :param kernel_size: 卷积核的长宽\n",
    "    :param stride: 步长\n",
    "    :return: y: 按卷积步骤展开后的矩阵\n",
    "    \"\"\"\n",
    "    from cupy.lib.stride_tricks import as_strided\n",
    "    B,C, h, w = x.shape\n",
    "    out_H, out_W = (h - kernel_size[0]) // stride[0] + 1, (w - kernel_size[1]) // stride[1] + 1\n",
    "    shape = (B,C, out_H, out_W, kernel_size[0], kernel_size[1])\n",
    "    strides = (*x.strides[:-2], x.strides[-2] * stride[0],\n",
    "               x.strides[-1] * stride[1], *x.strides[-2:])\n",
    "    y = as_strided(x, shape, strides=strides)\n",
    "    return y\n",
    "\n",
    "def conv(img, conv_filter):\n",
    "   \n",
    "    if len(img.shape)!=4 or len(conv_filter.shape)!=4:\n",
    "        print(\"卷积运算所输入的维度不符合要求\")\n",
    "        return 0\n",
    "        \n",
    "    if img.shape[-1] != conv_filter.shape[-1]:\n",
    "        print(\"卷积输入图片与卷积核的通道数不一致\")\n",
    "        return 0\n",
    "    img_2d = cp.copy(img)\n",
    "    img_2d = img_2d.transpose(0,3,1,2)\n",
    "    x_stride = split_by_strides(img_2d, conv_filter.shape[-3:-1], (1,1))*1\n",
    "    filter2 = conv_filter.transpose(0,3,1,2)\n",
    "    img_out = cp.tensordot(x_stride, filter2, axes=[(1, 4, 5), (1, 2, 3)]).transpose((0, 3, 1, 2))\n",
    "\n",
    "    return img_out.transpose(0,2,3,1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    return img_out\n",
    "def reverse_conv2d(x1, kernel1, rotate=False, invert=False):\n",
    "    \"\"\"\n",
    "    conv2d的反向卷积,求梯度时用的\n",
    "    @param x: 被卷积的张量\n",
    "    @param kernel: 卷积核\n",
    "    @param rotate: 卷积核旋转180度\n",
    "    @param invert: 该参数有点迷,不好解释,简单的说就是反向卷积有两种,视卷积结果的形状需要调整一些轴的位置\n",
    "    @return: 反向卷积结果\n",
    "    \"\"\"\n",
    "    kernel = kernel1.transpose(0,3,1,2)\n",
    "    x = x1.transpose(0,3,1,2)\n",
    "    ksize = kernel.shape\n",
    "    x = split_by_strides(x, ksize[-2:])\n",
    "    if rotate:\n",
    "        kernel = rotate180(kernel)\n",
    "    i = 0 if invert else 1\n",
    "    y = cp.tensordot(x, kernel, [(i, 4, 5), (0, 2, 3)])\n",
    "    if invert:\n",
    "        return y.transpose((3, 1, 2,0))/x1.shape[0]\n",
    "    return y.transpose((0, 3, 1, 2))\n",
    "def split_by_strides2(X, kh, kw, s):\n",
    "    N, H, W, C = X.shape\n",
    "    oh = (H - kh) // s + 1\n",
    "    ow = (W - kw) // s + 1\n",
    "    shape = (N, oh, ow, kh, kw, C)\n",
    "    strides = (X.strides[0], X.strides[1]*s, X.strides[2]*s, *X.strides[1:])\n",
    "    A = cp.lib.stride_tricks.as_strided(X, shape=shape, strides=strides)\n",
    "    return A\n",
    "def conv2(img, conv_filter):\n",
    "   \n",
    "    if len(img.shape)!=4 or len(conv_filter.shape)!=4:\n",
    "        print(\"卷积运算所输入的维度不符合要求\")\n",
    "        return 0\n",
    "        \n",
    "    if img.shape[-1] != conv_filter.shape[-1]:\n",
    "        print(\"卷积输入图片与卷积核的通道数不一致\")\n",
    "        return 0\n",
    "    img_num,img_h, img_w, img_ch = img.shape\n",
    "    filter_num, filter_h, filter_w, img_ch = conv_filter.shape\n",
    "    feature_h = img_h - filter_h + 1\n",
    "    feature_w = img_w - filter_w + 1\n",
    "    img_2d = cp.copy(img)   \n",
    "    x_stride = split_by_strides2(img_2d, filter_h, filter_w, 1)*1\n",
    "    filter2 = conv_filter.transpose(1,2,3,0)\n",
    "    img_out = cp.tensordot(x_stride, filter2, axes=[(3,4,5), (0,1,2)]) \n",
    "\n",
    "    return img_out\n",
    "import load_CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25ae5ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def turn_2_zero(x1):\n",
    "    import numpy\n",
    "    x = cp.asnumpy(x1)\n",
    "    y = numpy.int64(x>0)\n",
    "    return cp.asarray(y)\n",
    "## Data augmentation is necessary\n",
    "def flip_ran(data1,p,dim):\n",
    "    data = data1*1\n",
    "    ran = turn_2_zero(p*cp.ones((data.shape))-cp.array([cp.random.uniform(0,1)*cp.ones((xx.shape)) for xx in data] ))\n",
    "    data_b = ran*cp.flip(data*1,dim)+(1-ran)*data*1\n",
    "    return data_b\n",
    "def light_adjust(data1, a, b,p):\n",
    "    data = data1*1\n",
    "    temp = data*a+b\n",
    "    temp2 = temp*1\n",
    "    temp2[temp>1] = 1.0\n",
    "    ran = turn_2_zero(p*cp.ones((data.shape))-cp.array([cp.random.uniform(0,1)*cp.ones((xx.shape)) for xx in data] ))\n",
    "    data_b = ran*temp2+(1-ran)*data*1\n",
    "    return data_b\n",
    "def rotate(data1,p):\n",
    "    data = data1*1\n",
    "    temp2 = (cp.rot90(data, 1,axes=(1, 2)))\n",
    "    ran = turn_2_zero(p*cp.ones((data.shape))-cp.array([cp.random.uniform(0,1)*cp.ones((xx.shape)) for xx in data] ))\n",
    "    data_b = ran*temp2+(1-ran)*data*1\n",
    "    return data_b\n",
    "def Augmentation(data1,label1):\n",
    "    num = 4\n",
    "    data=cp.array(data1*1)\n",
    "    label=cp.array(label1*1)\n",
    "    lenth = data.shape[0]\n",
    "    datab = cp.zeros((lenth*num,data.shape[1],data.shape[2],data.shape[3]))\n",
    "    labelb = cp.zeros((lenth*num,label.shape[1],label.shape[2]))\n",
    "    datab[0:lenth] = data*1\n",
    "    datab[lenth:2*lenth] = flip_ran(data,0.7,2)\n",
    "    datab[2*lenth:3*lenth] = rotate(datab[lenth:2*lenth],0.7)\n",
    "    datab[3*lenth:4*lenth] = light_adjust(datab[2*lenth:3*lenth], cp.random.uniform(0.2,3.0), cp.random.uniform(0.0,1.5),0.7)\n",
    "    labelb[0:lenth] = label*1\n",
    "    labelb[1*lenth:2*lenth] = label*1\n",
    "    labelb[2*lenth:3*lenth] = label*1\n",
    "    labelb[3*lenth:4*lenth] = label*1\n",
    "    del data1,label1,data,label\n",
    "    return datab,labelb\n",
    "train_data,train_label,test_data,test_label=load_CIFAR10.data_generate('cifar-10-batches-py')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4449004",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(object):\n",
    "    def __init__(self,k):\n",
    "        channel = 3\n",
    "        '''\n",
    "        2层卷积，2层池化，3层全连接'''\n",
    "        self.filters = [cp.random.normal(0,cp.sqrt(2/(3*3*channel)),(k, 3, 3, channel))] \n",
    "        self.filters_biases = [cp.zeros((k,1))]\n",
    "        self.filters.append(cp.random.normal(0,cp.sqrt(2/(3*3*k)),(2*k, 3, 3, k)) )\n",
    "        self.filters_biases.append(cp.zeros((2*k,1)))\n",
    "        self.filters.append(cp.random.normal(0,cp.sqrt(2/(3*3*2*k)),(4*k, 3, 3, 2*k)) )\n",
    "        self.filters_biases.append(cp.zeros((4*k,1)))\n",
    "        self.filters.append(cp.random.normal(0,cp.sqrt(2/(3*3*4*k)),(8*k, 3, 3, 4*k)) )\n",
    "        self.filters_biases.append(cp.zeros((8*k,1)))\n",
    "        \n",
    "        self.weights = [cp.random.normal(0,cp.sqrt(2/(8*k)),(10,8*k))]\n",
    "        self.biases=[(cp.zeros((10,1)))]\n",
    "        \n",
    "        self.Adam_b= Adam(self.biases)\n",
    "        self.Adam_w = Adam(self.weights)\n",
    "        self.Adam_ft = Adam(self.filters)\n",
    "        self.Adam_fb = Adam(self.filters_biases)\n",
    "        self.BN_layer = [BN_layer(k*1,shape=2),BN_layer(2*k*1,shape=2),BN_layer(k*4,shape=2),BN_layer(k*8,shape=2)]\n",
    "\n",
    "\n",
    "    def Backward(self, x,y,activate,dactivate,evaluate=False,train1=True):\n",
    "        #第一层卷积\n",
    "        conv_all=[]\n",
    "        convy_all=[]\n",
    "        pool_all=[x*1]\n",
    "        pool_loc=[]\n",
    "        out=[]\n",
    "        pool_para = [1,2,2,8]              \n",
    "        for i in range(4):   \n",
    "            conv1 = add_bias(conv(padding(pool_all[i], 1), self.filters[i]),self.filters_biases[i])\n",
    "            conv1_y = self.BN_layer[i].BN_backward(conv1,None,0,train=train1)\n",
    "            relu1 = activate(conv1_y)\n",
    "            pool1, pool1_max_locate = pool(relu1, size=pool_para[i], stride=pool_para[i])#num*32*32*k\n",
    "            pool_all.append(pool1*1)\n",
    "            conv_all.append(conv1*1)\n",
    "            convy_all.append(conv1_y*1)\n",
    "            pool_loc.append(pool1_max_locate*1)\n",
    "            out.append(relu1*1)\n",
    "        straight_input = pool_all[-1].reshape(pool_all[-1].shape[0],pool_all[-1].shape[1] * pool_all[-1].shape[2] * pool_all[-1].shape[3])\n",
    "        active=[straight_input.T*1]#全连接层的输入数据\n",
    "        active.append(softmax(cp.dot(self.weights[0],active[-1]))*1)#输出层的数据\n",
    "        if evaluate == True:\n",
    "            return active[-1]\n",
    "        delta_full=[]\n",
    "        delta_full.append((active[-1]-cp.squeeze(y.T))*1)#最后一层\n",
    "        delta_full.append(cp.dot(self.weights[0].transpose(), delta_full[-1])*1)#第一层\n",
    "        delta_convx=[]\n",
    "        nabla_filters = [cp.zeros((b_s.shape)) for b_s in self.filters]\n",
    "        nabla_bias = [cp.zeros((b_s.shape)) for b_s in self.filters_biases]\n",
    "        \n",
    "        \n",
    "        for l in range(1, 5):\n",
    "            if (l==1):\n",
    "                delta_pool_ = (delta_full[-l].T).reshape(pool_all[-l].shape)\n",
    "                delta_convy_ = pool_delta_error_bp(delta_pool_, pool_loc[-l],size=pool_para[-l],stride =pool_para[-l]) * dactivate(convy_all[-l])\n",
    "                delta_convx_ = self.BN_layer[-l].BN_backward(conv_all[-l],delta_convy_,1,train=True)\n",
    "                delta_convx.append(delta_convx_*1)\n",
    "            else:\n",
    "                delta_pool_ = conv(padding(delta_convx[-1], self.filters[-l+1].shape[1]-2), rotate180(self.filters[-l+1]).swapaxes(0,3))\n",
    "                delta_convy_ = pool_delta_error_bp(delta_pool_, pool_loc[-l],size=pool_para[-l],stride =pool_para[-l]) * dactivate(convy_all[-l])\n",
    "                delta_convx_ = self.BN_layer[-l].BN_backward(conv_all[-l],delta_convy_,1,train=True)\n",
    "                delta_convx.append(delta_convx_*1)\n",
    "            nabla_filters[-l] = reverse_conv2d(padding(pool_all[-l-1],1),delta_convx_,rotate=False,invert = True) \n",
    "            nabla_bias[-l] = (conv_cal_b(delta_convx_))\n",
    "         \n",
    "        \n",
    "        nabla_w=[cp.zeros((b_s.shape)) for b_s in self.weights]\n",
    "        nabla_b=[cp.zeros((b_s.shape)) for b_s in self.biases]\n",
    "        for l in range(1):\n",
    "            nabla_w[-l] = (cp.dot(delta_full[l-2], active[l-2].transpose())/(cp.shape(x)[0]))#按照顺序来的\n",
    "            nabla_b[-l] = (cp.mean(delta_full[l-2],axis=1,keepdims=True))\n",
    " \n",
    "        \n",
    "\n",
    "        \n",
    "        if evaluate == False:\n",
    "            return nabla_w, nabla_b, nabla_filters, nabla_bias\n",
    "        \n",
    "    def evaluate(self, test_data1,test_label1,activate,dactivate):\n",
    "        # 获得预测结果a:10*batch_size\n",
    "        #testlabel:batch_size*10*1\n",
    "        test_data = test_data1*1 \n",
    "        test_label=test_label1*1\n",
    "        data,label = mini_batch_generate(100,test_data,test_label)\n",
    "        accuracy=[]\n",
    "        cross = 0\n",
    "        del test_data\n",
    "        del test_label\n",
    "        for i in range(data.shape[0]):\n",
    "            a=self.Backward(data[i],label[i],activate,dactivate,evaluate=True,train1=False)\n",
    "            max0=cp.argmax(a,axis=0).reshape(a.shape[1],1)\n",
    "            max1=cp.argmax(label[i],axis=1)\n",
    "            acc=((cp.sum((max0-max1) == 0))/(data[i].shape[0]))\n",
    "            accuracy.append(acc)\n",
    "            cross+=-cp.average(cp.sum((cp.squeeze(label[i]).T)*ln(a+pow(10,-30)),axis=0))\n",
    "        return cp.average(accuracy),cross/data.shape[0]\n",
    "        del data,label,a,max0,max1,acc\n",
    "        \n",
    "\n",
    "    def adam_update(self,lr,mini_batch_size,activate,dactivate,train_data_x,train_label_x):\n",
    "        data_x=train_data_x*1\n",
    "        label_x=train_label_x*1\n",
    "        data1,label1 = mini_batch_generate(mini_batch_size,data_x,label_x)\n",
    "        del data_x\n",
    "        del label_x\n",
    "        for j in range(data1.shape[0]):\n",
    "#             data,label = Augmentation(data1[j]*1,label1[j]*1)\n",
    "            delta_nabla_w, delta_nabla_b, delta_nabla_f, delta_nabla_fb =  self.Backward(data1[j],label1[j],activate,dactivate,evaluate=False)\n",
    "            self.weights = self.Adam_w.New_theta(self.weights,delta_nabla_w,lr)\n",
    "            self.biases =  self.Adam_b.New_theta(self.biases,delta_nabla_b,lr)\n",
    "            self.filters = self.Adam_ft.New_theta(self.filters,delta_nabla_f,lr)\n",
    "            self.filters_biases = self.Adam_fb.New_theta(self.filters_biases,delta_nabla_fb,lr)\n",
    "            print('\\r'+str(j)+'/'+str(int(data1.shape[0])),end='')\n",
    "    def SGD(self,mini_batch_size,epoch,lr0,activate,dactivate):\n",
    "        acc1_=[]\n",
    "        for i in range(epoch):\n",
    "            lr = divi_(lr0,i,40)\n",
    "            self.adam_update(lr,mini_batch_size,activate,dactivate,train_data,train_label)\n",
    "#             self.update_mini_batch(lr, mini_batch_size)\n",
    "            print (\"Epoch %s training complete\" % i)\n",
    "            acc1,cross = self.evaluate(test_data,test_label,activate,dactivate)\n",
    "            print(\"the test Accuracy for task is:{} %\".format((acc1)*100))\n",
    "            print(\"the test loss for task is:{} \".format((cross)*100))\n",
    "            acc1_.append(acc1*100)\n",
    "        return acc1_\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476ac3da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "408/500"
     ]
    }
   ],
   "source": [
    "net = ConvNet(64)\n",
    "net.SGD(100,200,0.001,relu,drelu)\n",
    "# import time\n",
    "# time1 = time.time()\n",
    "# net.evaluate(test_data,test_label,relu,drelu)\n",
    "# time2 = time.time()\n",
    "# print(time2-time1)\n",
    "# x=net.Backward(train_data[0:10],train_label[0:10],relu,drelu,evaluate=False)\n",
    "# acc=[]\n",
    "# for i in range(1):\n",
    "#     xx=net.SGD(100,200,0.001,relu,drelu)\n",
    "#     acc.append(xx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1833935a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
