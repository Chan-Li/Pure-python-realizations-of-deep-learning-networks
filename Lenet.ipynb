{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1418d1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as cp\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import log as ln\n",
    "import random\n",
    "from numpy import random\n",
    "import math\n",
    "from matplotlib.pyplot import plot,savefig\n",
    "import scipy\n",
    "from scipy import signal\n",
    "def relu(y):\n",
    "    tmp = y.copy()\n",
    "    tmp[tmp < 0] = 0\n",
    "    return tmp\n",
    "def drelu(x):\n",
    "    tmp = x.copy()\n",
    "    tmp[tmp >= 0] = 1\n",
    "    tmp[tmp < 0] = 0\n",
    "    return tmp\n",
    "\n",
    "def softmax(y):\n",
    "    y = y - cp.array(y.max(axis=0),ndmin=2)\n",
    "    exp_y = cp.exp(y) \n",
    "    sumofexp = cp.array(exp_y.sum(axis=0),ndmin=2)\n",
    "    softmax = exp_y/sumofexp\n",
    "    return softmax\n",
    "\n",
    "def divi_(lr0,global_step,decay_step):\n",
    "    return lr0*(0.5**((int(global_step/decay_step))))\n",
    "import sys\n",
    "filePath = 'Utils/nup'\n",
    "sys.path.append(filePath)\n",
    "import load_MNIST\n",
    "num =60000\n",
    "mnist=(load_MNIST.load_mnist(one_hot=True))\n",
    "train_data0 = mnist[0][0][0:num].reshape(num,28,28,1)\n",
    "train_label0 = mnist[0][1][0:num].reshape(num,10,1)\n",
    "test_data0 = mnist[1][0][0:10000].reshape(10000,28,28,1)\n",
    "test_label0 = mnist[1][1][0:10000].reshape(10000,10,1)\n",
    "def uni_permu(a,b,direction):\n",
    "    if direction ==1:\n",
    "        p = cp.random.permutation(len(a.T))\n",
    "        return cp.array((a.T[p]).T), cp.array((b.T[p]).T)\n",
    "    if direction == 0:\n",
    "        p = cp.random.permutation(len(a))\n",
    "        return cp.array((a[p])), cp.array((b[p]))\n",
    "def mini_batch_generate(mini_batch_size,data1,label1):\n",
    "    data = cp.array(data1*1)\n",
    "    label = cp.array(label1*1)\n",
    "    if (data.shape[1]%mini_batch_size == 0):\n",
    "        n=data.shape[1]\n",
    "    else:\n",
    "        n = (int(data.shape[1]/mini_batch_size))*mini_batch_size\n",
    "    data,label = uni_permu(data,label,1)\n",
    "    mini_batches = cp.array([data[:,k:k+mini_batch_size] for k in range(0,n,mini_batch_size)])\n",
    "    mini_batches_labels =cp.array([label[:,k:k+mini_batch_size] for k in range(0,n,mini_batch_size)])\n",
    "    return (mini_batches),(mini_batches_labels)\n",
    "def o_sigmoid(mat):\n",
    "    return (1.0/(1+pow(cp.e,-mat))).reshape(mat.shape)\n",
    "def sigmoid(mat):\n",
    "    return (2.0/(1+pow(cp.e,-mat))-1.0).reshape(mat.shape)\n",
    "def dsigmoid(mat):\n",
    "    return (o_sigmoid(mat)*(1-o_sigmoid(mat)))\n",
    "def padding(image, zero_num):\n",
    "    if len(image.shape) == 4:\n",
    "        image_padding = cp.zeros((image.shape[0],image.shape[1]+2*zero_num,image.shape[2]+2*zero_num,image.shape[3]))\n",
    "        image_padding[:,zero_num:image.shape[1]+zero_num,zero_num:image.shape[2]+zero_num,:] = image\n",
    "        return image_padding\n",
    "    elif len(image.shape) == 3:\n",
    "        image_padding = cp.zeros((image.shape[0]+2*zero_num, image.shape[1]+2*zero_num, image.shape[2]))\n",
    "        image_padding[zero_num:image.shape[0]+zero_num, zero_num:image.shape[1]+zero_num,:] = image\n",
    "        return image_padding\n",
    "    else:\n",
    "        print(\"维度错误\")\n",
    "        return 0\n",
    "    \n",
    "\n",
    "\n",
    "def pool(feature, size=2, stride=2):\n",
    "    num,feature_h, feature_w, feature_ch = feature.shape\n",
    "    pool_h = cp.uint16((feature_h - size)/stride + 1)\n",
    "    pool_w = cp.uint16((feature_w - size)/stride + 1)\n",
    "    feature_reshaped = feature.reshape(num,pool_h, feature_h//pool_h, pool_w, feature_w//pool_w, feature_ch)\n",
    "    out = feature_reshaped.max(axis=2).max(axis=3)\n",
    "    out_location_c = feature_reshaped.max(axis=2).argmax(axis=3)\n",
    "    out_location_r = feature_reshaped.max(axis=4).argmax(axis=2)\n",
    "    out_location = out_location_r * size + out_location_c\n",
    "    return out, out_location\n",
    "\n",
    "def rotate180(kernel, axis=(-2, -3)):\n",
    "    return cp.flip(kernel, axis)\n",
    "def add_bias(conv, bias):\n",
    "    if conv.shape[-1] != bias.shape[0]:\n",
    "        print(\"给卷积添加偏置维度出错\")\n",
    "    else:\n",
    "        for i in range(bias.shape[0]):\n",
    "            conv[:,:,:,i] += bias[i,0]\n",
    "    return conv\n",
    "def uni_permu(a1,b1,direction):\n",
    "    a=a1*1\n",
    "    b=b1*1\n",
    "    if direction ==1:\n",
    "        p = cp.random.permutation(len(a.T))\n",
    "        return cp.array((a.T[p]).T), cp.array((b.T[p]).T)\n",
    "    if direction == 0:\n",
    "        p = cp.random.permutation(len(a))\n",
    "        return cp.array((a[p])), cp.array((b[p]))\n",
    "def mini_batch_generate(mini_batch_size,data1,label1):\n",
    "    data = cp.array(data1*1).reshape(data1.shape[0],data1.shape[1]*data1.shape[2])\n",
    "    label = cp.array(label1*1).reshape(label1.shape[0],label1.shape[1]*label1.shape[2])\n",
    "    if (data.shape[0]%mini_batch_size == 0):\n",
    "        n=data.shape[0]\n",
    "    else:\n",
    "        n = (int(data.shape[0]/mini_batch_size))*mini_batch_size\n",
    "    data,label = uni_permu(data,label,0)\n",
    "    mini_batches = cp.array([data[k:k+mini_batch_size] for k in range(0,n,mini_batch_size)])\n",
    "    mini_batches_labels =cp.array([label[k:k+mini_batch_size] for k in range(0,n,mini_batch_size)])\n",
    "    mini_batches = mini_batches.reshape(int(data.shape[0]/mini_batch_size),mini_batch_size,data1.shape[1],data1.shape[2],1)\n",
    "    mini_batches_labels = mini_batches_labels.reshape(int(label.shape[0]/mini_batch_size),mini_batch_size,label.shape[1],1)\n",
    "    return (mini_batches),(mini_batches_labels)\n",
    "# def conv_(img, conv_filter):\n",
    "#     # 对二维图像以及二维卷积核进行卷积，不填充后对图像数平均\n",
    "#     num,img_h, img_w = img.shape\n",
    "#     num,filter_h, filter_w = conv_filter.shape\n",
    "#     feature_h = img_h - filter_h + 1\n",
    "#     feature_w = img_w - filter_w + 1\n",
    "#     img_matrix = cp.zeros((feature_h*feature_w, num*filter_h*filter_w))\n",
    "#     img_o=0\n",
    "#     for i in range(feature_h*feature_w):\n",
    "#         img_matrix[i,:] = img[:,cp.uint16(i/feature_w):cp.uint16(i/feature_w+filter_h),cp.uint16(i%feature_w):cp.uint16(i%feature_w+filter_w)].reshape(num*filter_w*filter_h)        \n",
    "#         filter_matrix = conv_filter.reshape(num*filter_h*filter_w,1)\n",
    "#         img_out = cp.dot(img_matrix, filter_matrix)\n",
    "#         img_out = img_out.reshape(feature_h,feature_w) \n",
    "#     return img_out/img.shape[0]\n",
    "\n",
    "# def conv_(img, conv_filter):\n",
    "#     # 对二维图像以及二维卷积核进行卷积，不填充后对图像数平均\n",
    "#     num,img_h, img_w = img.shape\n",
    "#     num,filter_h, filter_w = conv_filter.shape\n",
    "#     feature_h = img_h - filter_h + 1\n",
    "#     feature_w = img_w - filter_w + 1\n",
    "#     img_out =[]\n",
    "#     for i in range(num):\n",
    "#         img_out.append(scipy.signal.convolve2d(image[i], np.fliplr(np.flipud(conv_filter[i])),\n",
    "#                               mode = \"valid\")*1)\n",
    "#     return np.average(img_out,axis=0)\n",
    "\n",
    "def conv_cal_w(out_img_delta, in_img):\n",
    "    #由卷积前的图片以及卷积后的delta计算卷积核的梯度\n",
    "    #out:(500, 10, 10, 16),in_img: (500, 14, 14, 6)\n",
    "    nabla_conv = cp.zeros([out_img_delta.shape[-1], in_img.shape[1]-out_img_delta.shape[1]+1, \n",
    "                           in_img.shape[2]-out_img_delta.shape[2]+1, in_img.shape[-1]])\n",
    "    for filter_num in range(nabla_conv.shape[0]):\n",
    "        for ch_num in range(nabla_conv.shape[-1]):\n",
    "            nabla_conv[filter_num,:,:,ch_num] = conv_(in_img[:,:,:,ch_num], out_img_delta[:,:,:,filter_num])\n",
    "    return nabla_conv\n",
    "\n",
    "def conv_cal_b(out_img_delta):\n",
    "    #shape: 500, 10, 10, 16\n",
    "    return cp.average(out_img_delta,axis = (0,1,2)).reshape(out_img_delta.shape[-1],1)\n",
    "\n",
    "        \n",
    "def pool_delta_error_bp(pool_out_delta1, pool_out_max_location1, size=2, stride=2):\n",
    "    pool_out_delta = pool_out_delta1*1\n",
    "    pool_out_max_location = pool_out_max_location1*1\n",
    "    num,pool_h, pool_w, pool_ch = pool_out_delta.shape\n",
    "    in_h = cp.uint16((pool_h-1)*stride+size)\n",
    "    in_w = cp.uint16((pool_w-1)*stride+size)\n",
    "    in_ch = pool_ch\n",
    "    #复原尺寸\n",
    "    pool_out_delta_reshaped = pool_out_delta.transpose(0,3,1,2)#16*5*5\n",
    "    pool_out_delta_reshaped = pool_out_delta_reshaped.reshape(num*pool_h*pool_w*pool_ch)\n",
    "    \n",
    "    pool_out_max_location_reshaped = pool_out_max_location.transpose(0,3,1,2)\n",
    "    pool_out_max_location_reshaped = pool_out_max_location_reshaped.reshape(num*pool_h*pool_w*pool_ch)\n",
    "    \n",
    "    in_delta_matrix = cp.zeros([num*pool_h*pool_w*pool_ch,size*size])\n",
    "    \n",
    "    in_delta_matrix[cp.arange(num*pool_h*pool_w*pool_ch), pool_out_max_location_reshaped] = pool_out_delta_reshaped\n",
    "    \n",
    "    in_delta = in_delta_matrix.reshape(num,pool_ch,pool_h, pool_w, size, size)\n",
    "    in_delta = in_delta.transpose(0,2,4,3,5,1)\n",
    "    in_delta = in_delta.reshape(num,in_h, in_w, in_ch)\n",
    "    return  in_delta\n",
    "\n",
    "train_image = padding(train_data0,2)#对初始图像进行零填充，保证与LeNet输入结构一致60000*32*32*1\n",
    "test_image = padding(test_data0,2)\n",
    "\n",
    "def split_by_strides2(X, kh, kw, s):\n",
    "    N, H, W, C = X.shape\n",
    "    oh = (H - kh) // s + 1\n",
    "    ow = (W - kw) // s + 1\n",
    "    shape = (N, oh, ow, kh, kw, C)\n",
    "    strides = (X.strides[0], X.strides[1]*s, X.strides[2]*s, *X.strides[1:])\n",
    "    A = cp.lib.stride_tricks.as_strided(X, shape=shape, strides=strides)\n",
    "    return A\n",
    "\n",
    "def split_by_strides(x, kernel_size, stride=(1, 1)):\n",
    "    \"\"\"\n",
    "    将张量按卷积核尺寸与步长进行分割\n",
    "    :param x: 被卷积的张量\n",
    "    :param kernel_size: 卷积核的长宽\n",
    "    :param stride: 步长\n",
    "    :return: y: 按卷积步骤展开后的矩阵\n",
    "    \"\"\"\n",
    "    from numpy.lib.stride_tricks import as_strided\n",
    "    B,C, h, w = x.shape\n",
    "    out_H, out_W = (h - kernel_size[0]) // stride[0] + 1, (w - kernel_size[1]) // stride[1] + 1\n",
    "    shape = (B,C, out_H, out_W, kernel_size[0], kernel_size[1])\n",
    "    strides = (*x.strides[:-2], x.strides[-2] * stride[0],\n",
    "               x.strides[-1] * stride[1], *x.strides[-2:])\n",
    "    y = as_strided(x, shape, strides=strides)\n",
    "    return y\n",
    "\n",
    "def conv(img, conv_filter):\n",
    "   \n",
    "    if len(img.shape)!=4 or len(conv_filter.shape)!=4:\n",
    "        print(\"卷积运算所输入的维度不符合要求\")\n",
    "        return 0\n",
    "        \n",
    "    if img.shape[-1] != conv_filter.shape[-1]:\n",
    "        print(\"卷积输入图片与卷积核的通道数不一致\")\n",
    "        return 0\n",
    "    img_2d = cp.copy(img)\n",
    "    img_2d = img_2d.transpose(0,3,1,2)\n",
    "    x_stride = split_by_strides(img_2d, conv_filter.shape[-3:-1], (1,1))*1\n",
    "    filter2 = conv_filter.transpose(0,3,1,2)\n",
    "    img_out = cp.tensordot(x_stride, filter2, axes=[(1, 4, 5), (1, 2, 3)]).transpose((0, 3, 1, 2))\n",
    "\n",
    "    return img_out.transpose(0,2,3,1)\n",
    "\n",
    "\n",
    "\n",
    "def conv2(img, conv_filter):\n",
    "   \n",
    "    if len(img.shape)!=4 or len(conv_filter.shape)!=4:\n",
    "        print(\"卷积运算所输入的维度不符合要求\")\n",
    "        return 0\n",
    "        \n",
    "    if img.shape[-1] != conv_filter.shape[-1]:\n",
    "        print(\"卷积输入图片与卷积核的通道数不一致\")\n",
    "        return 0\n",
    "    img_num,img_h, img_w, img_ch = img.shape\n",
    "    filter_num, filter_h, filter_w, img_ch = conv_filter.shape\n",
    "    feature_h = img_h - filter_h + 1\n",
    "    feature_w = img_w - filter_w + 1\n",
    "    img_2d = cp.copy(img)   \n",
    "    x_stride = split_by_strides2(img_2d, filter_h, filter_w, 1)*1\n",
    "    filter2 = conv_filter.transpose(1,2,3,0)\n",
    "    img_out = cp.tensordot(x_stride, filter2, axes=[(3,4,5), (0,1,2)]) \n",
    "\n",
    "    return img_out\n",
    "def reverse_conv2d(x1, kernel1, rotate=False, invert=False):\n",
    "    \"\"\"\n",
    "    conv2d的反向卷积,求梯度时用的\n",
    "    @param x: 被卷积的张量\n",
    "    @param kernel: 卷积核\n",
    "    @param rotate: 卷积核旋转180度\n",
    "    @param invert: 该参数有点迷,不好解释,简单的说就是反向卷积有两种,视卷积结果的形状需要调整一些轴的位置\n",
    "    @return: 反向卷积结果\n",
    "    \"\"\"\n",
    "    kernel = kernel1.transpose(0,3,1,2)\n",
    "    x = x1.transpose(0,3,1,2)\n",
    "    ksize = kernel.shape\n",
    "    x = split_by_strides(x, ksize[-2:])\n",
    "    if rotate:\n",
    "        kernel = rotate180(kernel)\n",
    "    i = 0 if invert else 1\n",
    "    y = cp.tensordot(x, kernel, [(i, 4, 5), (0, 2, 3)])\n",
    "    if invert:\n",
    "        return y.transpose((3, 1, 2,0))/x1.shape[0]\n",
    "    return y.transpose((0, 3, 1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4449004",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adam:\n",
    "    def __init__(self,theta):\n",
    "        self.lr=0.01\n",
    "        self.beta1=0.9\n",
    "        self.beta2=0.999\n",
    "        self.epislon=1e-8\n",
    "        self.m=[cp.zeros(ms.shape) for ms in theta]\n",
    "        self.s=[cp.zeros(ms.shape) for ms in theta]\n",
    "        self.t=0\n",
    "    \n",
    "    def New_theta(self,theta,gradient,eta):\n",
    "        self.t += 1\n",
    "        self.lr = eta*1\n",
    "        self.decay=1e-4\n",
    "        g=gradient*1\n",
    "        theta2 = [cp.zeros(ms.shape) for ms in theta]\n",
    "        for l in range(len(gradient)):\n",
    "            self.m[l] = self.beta1*self.m[l] + (1-self.beta1)*g[l]\n",
    "            self.s[l] = self.beta2*self.s[l] + (1-self.beta2)*(g[l]*g[l])\n",
    "            self.mhat = self.m[l]/(1-self.beta1**self.t)\n",
    "            self.shat = self.s[l]/(1-self.beta2**self.t)\n",
    "            theta2[l] = theta[l]-self.lr*((self.mhat/(pow(self.shat,0.5)+self.epislon))+self.decay*theta[l])\n",
    "        return theta2*1\n",
    "class ConvNet(object):\n",
    "    def __init__(self):\n",
    "        channel = 1\n",
    "        '''\n",
    "        2层卷积，2层池化，3层全连接'''\n",
    "        self.filters = [cp.random.normal(0,0.1,(6, 5, 5, channel))] \n",
    "        self.filters_biases = [cp.zeros((6,channel))]\n",
    "        self.filters.append(cp.random.normal(0,0.1,(16, 5, 5, 6)) )\n",
    "        self.filters_biases.append(cp.zeros((16,channel)))\n",
    "        self.weights = [cp.random.normal(0,0.1,(120,400))]\n",
    "        self.weights.append(cp.random.normal(0,0.1,(84,120)))\n",
    "        self.weights.append(cp.random.normal(0,0.1,(10,84)))\n",
    "        self.biases = [0.1*cp.ones((120,1))]\n",
    "        self.biases.append(0.1*cp.ones((84,1)))\n",
    "        self.biases.append(0.1*cp.ones((10,1)))\n",
    "        self.Adam_b= Adam(self.biases)\n",
    "        self.Adam_w = Adam(self.weights)\n",
    "        self.Adam_ft = Adam(self.filters)\n",
    "        self.Adam_fb = Adam(self.filters_biases)\n",
    "    def Backward(self, x,y,activate,dactivate,evaluate=False):\n",
    "        #第一层卷积\n",
    "        conv_all=[]\n",
    "        pool_all=[]\n",
    "        pool_loc=[]\n",
    "        conv1 = add_bias(conv(x, self.filters[0]),self.filters_biases[0] )\n",
    "        relu1 = activate(conv1)\n",
    "        pool1, pool1_max_locate = pool(relu1)#num*14*14*6\n",
    "        conv2 = add_bias(conv(pool1, self.filters[1]), self.filters_biases[1])\n",
    "        relu2 = activate(conv2)\n",
    "        pool2, pool2_max_locate = pool(relu2) #num*5*5*16\n",
    "        straight_input = pool2.reshape(pool2.shape[0],pool2.shape[1] * pool2.shape[2] * pool2.shape[3])\n",
    "        zm=[]\n",
    "        active=[straight_input.T*1]\n",
    "        for l in range(3):\n",
    "            full_connect_z = cp.dot(self.weights[l], active[l]) + self.biases[l]\n",
    "            zm.append(full_connect_z)\n",
    "            if l == 2:\n",
    "                full_connect_a = softmax(full_connect_z)\n",
    "                active.append(full_connect_a)\n",
    "            else:\n",
    "                full_connect_a = activate(full_connect_z)          \n",
    "                active.append(full_connect_a)\n",
    "        delta_full=[]\n",
    "        nabla_w=[cp.zeros(w_s.shape) for w_s in self.weights]\n",
    "        nabla_bias=[cp.zeros(b_s.shape) for b_s in self.biases]\n",
    "        for l in range(1, 4):\n",
    "            if l == 1:\n",
    "                delta = active[-1]-cp.squeeze(y.T)\n",
    "                delta_full.append(delta)\n",
    "            else:\n",
    "                delta = cp.dot(self.weights[-l+1].transpose(), delta_full[-1]) * dactivate(zm[-l])\n",
    "                delta_full.append(delta)\n",
    "            nabla_w[-l] = (cp.dot(delta,(active[-l-1].T)))/(cp.shape(x)[0])\n",
    "            nabla_bias[-l] = (cp.sum(delta,axis=1,keepdims=True)/(cp.shape(x)[0]))\n",
    "            \n",
    "        delta_full.append(cp.dot(self.weights[0].transpose(), delta_full[-1]))\n",
    "        delta_pool2 = (delta_full[-1].T).reshape(pool2.shape)#(100, 5, 5, 16)\n",
    "        delta_conv2 = pool_delta_error_bp(delta_pool2, pool2_max_locate) * dactivate(conv2)\n",
    "        delta_pool1 = conv(padding(delta_conv2, self.filters[1].shape[1]-1), rotate180(self.filters[1]).swapaxes(0,3))* dactivate(pool1)\n",
    "        delta_conv1 = pool_delta_error_bp(delta_pool1, pool1_max_locate) * dactivate(conv1)\n",
    "        nabla_filters1 = reverse_conv2d(pool1,delta_conv2,rotate=False,invert = True) \n",
    "        nabla_filters0 = reverse_conv2d(x,delta_conv1,rotate=False,invert = True)\n",
    "        nabla_filters_biases1 = conv_cal_b(delta_conv2)\n",
    "        nabla_filters_biases0 = conv_cal_b(delta_conv1)\n",
    "        nabla_f = [nabla_filters0, nabla_filters1]\n",
    "        nabla_fb = [nabla_filters_biases0, nabla_filters_biases1]\n",
    "\n",
    "        if evaluate == True:\n",
    "            return active[-1]\n",
    "        if evaluate == False:\n",
    "            return nabla_w, nabla_bias, nabla_f, nabla_fb\n",
    "        \n",
    "    def evaluate(self, test_data1,test_label1,activate,dactivate):\n",
    "        # 获得预测结果a:10*batch_size\n",
    "        #testlabel:batch_size*10*1\n",
    "        test_data = test_data1*1 \n",
    "        test_label=test_label1*1\n",
    "        data,label = mini_batch_generate(500,test_data,test_label)\n",
    "        accuracy=[]\n",
    "        for i in range(data.shape[0]):\n",
    "            a=self.Backward(data[i],label[i],activate,dactivate,evaluate=True)\n",
    "            max0=cp.argmax(a,axis=0).reshape(a.shape[1],1)\n",
    "            max1=cp.argmax(label[i],axis=1)\n",
    "            acc=((cp.sum((max0-max1) == 0))/(data[i].shape[0]))\n",
    "            accuracy.append(acc)\n",
    "        return cp.average(accuracy)\n",
    "    def update_mini_batch(self,  eta, mini_batch_size):\n",
    "        '''通过一个batch的数据对神经网络参数进行更新\n",
    "        需要先求这个batch中每张图片的误差反向传播求得的权重梯度以及偏置梯度'''\n",
    "        data_x=train_image*1\n",
    "        label_x=train_label0*1\n",
    "        data,label = mini_batch_generate(mini_batch_size,data_x,label_x)\n",
    "        for j in range(data.shape[0]):\n",
    "            nabla_b = [cp.zeros(b.shape) for b in self.biases]\n",
    "            nabla_w = [cp.zeros(w.shape) for w in self.weights]\n",
    "        \n",
    "            nabla_f = [cp.zeros(f.shape) for f in self.filters]\n",
    "            nabla_fb = [cp.zeros(fb.shape) for fb in self.filters_biases]\n",
    "\n",
    "            delta_nabla_w, delta_nabla_b, delta_nabla_f, delta_nabla_fb = self.Backward(data[j],label[j],relu,drelu,evaluate=False)\n",
    "            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "            nabla_f = [nf+dnf for nf, dnf in zip(nabla_f, delta_nabla_f)]\n",
    "            nabla_fb = [nfb + dnfb for nfb, dnfb in zip(nabla_fb, delta_nabla_fb)]\n",
    "            self.weights = [w-(eta/mini_batch_size)*nw for w, nw in zip(self.weights, nabla_w)]\n",
    "            self.biases = [b-(eta/mini_batch_size)*nb for b, nb in zip(self.biases, nabla_b)]\n",
    "            self.filters = [f-(eta/mini_batch_size)*nf for f, nf in zip(self.filters, nabla_f)]\n",
    "            self.filters_biases = [fb-(eta/mini_batch_size)*nfb for fb, nfb in zip(self.filters_biases, nabla_fb)]\n",
    "            print('\\r'+str(j)+'/'+str(int(data.shape[0])),end='')\n",
    "    def adam_update(self,lr,mini_batch_size,activate,dactivate,train_data_x,train_label_x):\n",
    "        data_x=train_data_x*1\n",
    "        label_x=train_label_x*1\n",
    "        data,label = mini_batch_generate(mini_batch_size,data_x,label_x)\n",
    "        for j in range(data.shape[0]):\n",
    "            delta_nabla_w, delta_nabla_b, delta_nabla_f, delta_nabla_fb =  self.Backward(data[j],label[j],activate,dactivate,evaluate=False)\n",
    "            self.weights = self.Adam_w.New_theta(self.weights,delta_nabla_w,lr)\n",
    "            self.biases =  self.Adam_b.New_theta(self.biases,delta_nabla_b,lr)\n",
    "            self.filters = self.Adam_ft.New_theta(self.filters,delta_nabla_f,lr)\n",
    "            self.filters_biases = self.Adam_fb.New_theta(self.filters_biases,delta_nabla_fb,lr)\n",
    "            print('\\r'+str(j)+'/'+str(int(data.shape[0])),end='')\n",
    "    def SGD(self,mini_batch_size,epoch,lr0,activate,dactivate):\n",
    "        acc1_=[]\n",
    "        for i in range(epoch):\n",
    "            train_labelt=train_label0*1#改参数\n",
    "            train_datat=train_image*1\n",
    "            lr = divi_(lr0,i,40)\n",
    "            self.adam_update(lr,mini_batch_size,activate,dactivate,train_datat,train_labelt)\n",
    "#             self.update_mini_batch(lr, mini_batch_size)\n",
    "            print (\"Epoch %s training complete\" % i)\n",
    "            acc1 = self.evaluate(test_image,test_label0,activate,dactivate)\n",
    "            print(\"the test Accuracy for task0 is:{} %\".format((acc1)*100))\n",
    "            acc1_.append(acc1*100)\n",
    "        return acc1_\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403d866d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "599/600Epoch 0 training complete\n",
      "the test Accuracy for task0 is:97.33000000000001 %\n",
      "55/600"
     ]
    }
   ],
   "source": [
    "net = ConvNet()\n",
    "xx=net.SGD(100,120,0.001,relu,drelu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476ac3da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
